{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID2214 Assignment 2 Group no. [enter]\n",
    "### Project members: \n",
    "[Enter Name, email]\n",
    "[Enter Name, email]\n",
    "[Enter Name, email]\n",
    "[Enter Name, email]\n",
    "\n",
    "### Declaration:\n",
    "By submitting this solution, it is hereby declared that all individuals listed above have contributed to the solution, either with code that appear in the final solution below, or with code that has been evaluated and compared to the final solution, but for some reason has been excluded. It is also declared that all project members fully understand all parts of the final solution and can explain it upon request.\n",
    "\n",
    "It is furthermore declared that the code below is a contribution by the project members only, and specifically that no part of the solution has been copied from any other source (except for lecture slides at the course ID2214) and no part of the solution has been provided by someone not listed as project member above.\n",
    "\n",
    "It is furthermore declared that it has been understood that no other library/package than the Python 3 standard library, NumPy, pandas and time may be used in the solution for this assignment.\n",
    "\n",
    "\n",
    "### Instructions\n",
    "All assignments starting with number 1 below are mandatory. Satisfactory solutions\n",
    "will give 1 point (in total). If they in addition are good (all parts work more or less \n",
    "as they should), completed on time (submitted before the deadline in Canvas) and according\n",
    "to the instructions, together with satisfactory solutions of assignments starting with \n",
    "number 2 below, then the assignment will receive 2 points (in total).\n",
    "\n",
    "It is highly recommended that you do not develop the code directly within the notebook\n",
    "but that you copy the comments and test cases to your regular development environment\n",
    "and only when everything works as expected, that you paste your functions into this\n",
    "notebook, do a final testing (all cells should succeed) and submit the whole notebook \n",
    "(a single file) in Canvas (do not forget to fill in your group number and names above,\n",
    "and thereby \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NumPy, pandas and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reused functions from Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste functions from Assignment 1 here that you need for this assignment\n",
    "\n",
    "def create_normalization(df_init, normalizationtype = 'minmax'):\n",
    "    df = df_init.copy()\n",
    "    normalization = {}\n",
    "    for col in df.columns:\n",
    "        if(col not in ['ID', 'CLASS'] and df[col].dtype in ['float64', 'float32', 'int']):\n",
    "            if(normalizationtype == 'minmax'):\n",
    "                min = df[col].min()\n",
    "                max = df[col].max()\n",
    "                diff = max-min\n",
    "                df[col] = df[col].apply(lambda x: (x-min)/(diff))\n",
    "                normalization[col] = ('minmax', min, max)\n",
    "                \n",
    "            elif(normalizationtype == 'zscore'):\n",
    "                mean = df[col].mean()\n",
    "                std = df[col].std()\n",
    "                df[col] = df[col].apply(lambda x: (x-mean)/(std))\n",
    "                normalization[col] = ('zscore', mean, std)\n",
    "              \n",
    "    return(df, normalization)\n",
    "\n",
    "def apply_normalization(df_init, normalization):\n",
    "    df = df_init.copy()\n",
    "    for col in df.columns:\n",
    "        if(col not in ['ID', 'CLASS'] and df[col].dtype in ['float64', 'float32', 'int']):\n",
    "            normalizationtype, arg1, arg2 = normalization[col]\n",
    "            \n",
    "            if(normalizationtype == 'minmax'):\n",
    "                diff = arg2-arg1\n",
    "                #min(max()) to limit the output range to [0,1], see hint 2\n",
    "                df[col] = df[col].apply(lambda x: min(max((x-arg1)/(diff), 0),1))\n",
    "                \n",
    "            elif(normalizationtype == 'zscore'):\n",
    "                df[col] = df[col].apply(lambda x: (x-arg1)/(arg2))\n",
    "              \n",
    "    return(df)\n",
    "\n",
    "def create_imputation(df_init):\n",
    "    df = df_init.copy()\n",
    "    imputation = {}\n",
    "    nrow = df.shape[0]\n",
    "    for col in df.columns:\n",
    "        type_col = df[col].dtype\n",
    "        \n",
    "        #if colonne to impute as number\n",
    "        if(col not in ['ID', 'CLASS'] and type_col in ['float64', 'float32', 'int64', 'int32']):\n",
    "            na = np.sum(df[col].isna())\n",
    "            if(na == nrow):\n",
    "                mean = 0\n",
    "            else:\n",
    "                mean = df[col].mean()\n",
    "            df[col].fillna(mean, inplace=True)\n",
    "            imputation[col] = mean\n",
    "            \n",
    "        #if column not to impute as object or category\n",
    "        elif(col not in [\"ID\",\"CLASS\"]):                        \n",
    "            na = np.sum(df[col].isna())\n",
    "            if(na == nrow):\n",
    "                if(type_col == 'object'):\n",
    "                    mode = ''\n",
    "                elif(type_col == 'category'):\n",
    "                    mode = df[col].cat.categories[0]\n",
    "            else:\n",
    "                mode = df[col].mode()[0]\n",
    "            df[col].fillna(mode, inplace=True)\n",
    "            imputation[col] = mode\n",
    "            \n",
    "    return(df, imputation)\n",
    "\n",
    "def apply_imputation(df_init, imputation):\n",
    "    df = df_init.copy()\n",
    "    for col in df.columns:\n",
    "        if(col not in ['ID','CLASS']):                                      \n",
    "            df[col].fillna(imputation[col], inplace=True)            \n",
    "    return(df)\n",
    "\n",
    "def create_bins(df_init, nobins = 10, bintype = 'equal-width'):\n",
    "    df = df_init.copy()\n",
    "    binning = {}\n",
    "    for col in df.columns:\n",
    "        type_col = df[col].dtype\n",
    "        \n",
    "        if(col not in ['ID', 'CLASS'] and type_col in ['float64', 'float32', 'int64', 'int32']):\n",
    "            if(bintype == 'equal-width'):\n",
    "                df[col], bins = pd.cut(df[col],nobins,retbins=True, labels = False)\n",
    "            else:\n",
    "                # duplicate = 'drop' in case there are duplicates in the edges\n",
    "                df[col], bins = pd.qcut(df[col],nobins,retbins=True, labels = False, duplicates='drop')\n",
    "            bins[0], bins[-1] = -np.inf, np.inf\n",
    "            binning[col] = bins\n",
    "            df[col] = df[col].astype('category')\n",
    "            df[col] = df[col].cat.set_categories([str(i) for i in df[col].cat.categories], rename = True)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    return(df, binning)\n",
    "\n",
    "def apply_bins(df_init, binning):\n",
    "    df = df_init.copy()\n",
    "    for col in df.columns:\n",
    "        type_col = df[col].dtype\n",
    "        \n",
    "        if(col not in ['ID', 'CLASS'] and type_col in ['float64', 'float32', 'int64', 'int32']):\n",
    "            bins = binning[col]\n",
    "            df[col] = pd.cut(df[col],bins,labels=False)\n",
    "            df[col] = df[col].astype('category')\n",
    "            df[col] = df[col].cat.set_categories([str(i) for i in df[col].cat.categories], rename = True) \n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    return(df)\n",
    "\n",
    "def split(df_init, testfraction = 0.5):\n",
    "    n = df_init.shape[0]\n",
    "    n_test = int(n * testfraction)\n",
    "    indexes = [i for i in range(n)]\n",
    "    np.random.shuffle(indexes)\n",
    "    test_idx = indexes[:n_test]\n",
    "    training_idx = indexes[n_test:]\n",
    "    return(df_init.iloc[training_idx], df_init.iloc[test_idx])\n",
    "\n",
    "def accuracy(predictions, correctlabels):\n",
    "    correct = 0\n",
    "    n = predictions.shape[0]\n",
    "    for i in range(n):\n",
    "        d = predictions.iloc[i]\n",
    "        pred  = d.idxmax()\n",
    "        if(pred == correctlabels[i]):\n",
    "            correct += 1\n",
    "    return(correct/n)\n",
    "\n",
    "def create_one_hot(df_init):\n",
    "    df = df_init.copy()\n",
    "    df2 = df.copy()\n",
    "    one_hot = {}\n",
    "    for col in df.columns:\n",
    "        if((hasattr(df[col], 'object') or hasattr(df[col], 'category')) and col not in ['ID','CLASS']): \n",
    "            df[col] = df[col].astype('category')\n",
    "            n_cat = len(df[col].cat.categories)\n",
    "            one_hot[col] = df[col].cat.categories\n",
    "            for i in one_hot[col]:\n",
    "                name = col+'_'+i   \n",
    "                new_col = df[col]==i\n",
    "                new_col = new_col.astype('float')\n",
    "                df2[name]=new_col \n",
    "            df2 = df2.drop(columns = col, axis = 1) \n",
    "    return(df2, one_hot)\n",
    "\n",
    "def apply_one_hot(df_init, one_hot):\n",
    "    df = df_init.copy()\n",
    "    df2 = df.copy()\n",
    "    for col in df.columns:\n",
    "        if(col in one_hot.keys()):\n",
    "            for i in one_hot[col]:\n",
    "                name = col+'-'+i\n",
    "                new_col = df[col]==i\n",
    "                new_col = pd.Series(new_col.astype('float'))\n",
    "                df2[name] = new_col\n",
    "            df2 = df2.drop(columns = col, axis = 1)\n",
    "            \n",
    "    return(df2)\n",
    "\n",
    "def folds(df, nofolds):\n",
    "    shuffling = np.random.permutation(df.index)\n",
    "    intervals = [int(i*df.shape[0]/nofolds) for i in range(nofolds+1)]\n",
    "    listDf = [df.iloc[intervals[i]:intervals[i+1],:] for i in range(nofolds)]\n",
    "    return(listDf)\n",
    "\n",
    "def brier_score(df, correctlabels):\n",
    "    n = df.shape[0]\n",
    "    avg_error = 0\n",
    "    for i in range(n):\n",
    "        v = np.array(df.iloc[i,:])\n",
    "        idx = np.where(df.columns==correctlabels[i])[0]\n",
    "        correct = [1 if i == idx else 0 for i in range(df.shape[1])]\n",
    "        error = np.sum((v-correct)**2)\n",
    "        avg_error += error\n",
    "    avg_error = avg_error / n\n",
    "    return(avg_error)\n",
    "\n",
    "def get_true_false_positive(prediction_vector, label_vector, label):\n",
    "    pred = np.array(prediction_vector)\n",
    "    v = [i == label for i in label_vector] \n",
    "    this_label, not_this_label = pred[v], pred[[not i for i in v]]\n",
    "    return(this_label, not_this_label)\n",
    "\n",
    "def get_list(prediction_vector, label_vector, label):\n",
    "    this_label, not_this_label = get_true_false_positive(prediction_vector, label_vector, label)\n",
    "    scores_false = [0]\n",
    "    scores_false += sorted(prediction_vector)\n",
    "    scores_false += [1]\n",
    "    dict_scores = {}\n",
    "    for i in scores_false:\n",
    "        false_pos_i_r = np.sum(not_this_label>=i)/len(not_this_label)\n",
    "        true_pos_i_r = np.sum(this_label>=i)/len(this_label)\n",
    "        dict_scores[i] = [false_pos_i_r, true_pos_i_r]\n",
    "    list_reversed = [i for i in reversed(list(dict_scores.values()))]\n",
    "    return(list_reversed)\n",
    "\n",
    "def get_area(list_values):\n",
    "    n = len(list_values)\n",
    "    area = 0\n",
    "    for i in range(n-1):\n",
    "        left, right = i, i+1\n",
    "        tpr_left, tpr_right = list_values[left][1], list_values[right][1]\n",
    "        fpr_left, fpr_right = list_values[left][0], list_values[right][0]\n",
    "        if(fpr_right==fpr_left):\n",
    "            next\n",
    "        height = (tpr_left+tpr_right)/2\n",
    "        width = fpr_right-fpr_left\n",
    "        area += height*width\n",
    "    return(area)\n",
    "\n",
    "def get_frequencies(correctlabels):\n",
    "    x = pd.Series(correctlabels)\n",
    "    frequencies = x.value_counts()/len(correctlabels)\n",
    "    return(frequencies)\n",
    "    \n",
    "def auc(df, correctlabels):\n",
    "    frequencies = get_frequencies(correctlabels)\n",
    "    area = 0\n",
    "    for col in df.columns:\n",
    "        prediction_vector = df[col]\n",
    "        l = get_list(prediction_vector, correctlabels, col)\n",
    "        area_col = get_area(l)\n",
    "        area += frequencies[col]*area_col\n",
    "    return(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the class kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to __init__: \n",
    "# self: the object itself\n",
    "#\n",
    "# Output from __init__:\n",
    "# nothing\n",
    "# \n",
    "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
    "# imputation, normalization, one_hot, labels, training_labels, training_data\n",
    "#\n",
    "# Input to fit:\n",
    "# self: the object itself\n",
    "# df: a dataframe (where the column names \"CLASS\" and \"ID\" have special meaning)\n",
    "# normalizationtype: \"minmax\" (default) or \"zscore\"\n",
    "#\n",
    "# Output from fit:\n",
    "# nothing\n",
    "#\n",
    "# The result of applying this function should be:\n",
    "#\n",
    "# self.imputation should be an imputation mapping (see Assignment 1) from df\n",
    "# self.normalization should be a normalization mapping (see Assignment 1), using normalizationtype from the imputed df\n",
    "# self.one_hot should be a one-hot mapping (see Assignment 1; can be excluded if this function was not completed)\n",
    "# self.training_labels should be a pandas series corresponding to the \"CLASS\" column, set to be of type \"category\" \n",
    "# self.labels should be the categories of the previous series\n",
    "# self.training_data should be the values (an ndarray) of the transformed dataframe, i.e., after employing imputation, \n",
    "# normalization, and possibly one-hot encoding, and also after removing the \"CLASS\" and \"ID\" columns \n",
    "# Note that the function does not return anything but just assigns values to the attributes of the object.\n",
    "#\n",
    "# Input to predict:\n",
    "# self: the object itself\n",
    "# df: a dataframe\n",
    "# k: an integer >= 1 (default = 5)\n",
    "# \n",
    "# Output from predict:\n",
    "# predictions: a dataframe with class labels as column names and the rows corresponding to\n",
    "#              predictions with estimated class probabilities for each row in df, where the class probabilities\n",
    "#              are estimated by the relative class frequencies in the set of class labels from the k nearest \n",
    "#              (with respect to Euclidean distance) neighbors in training_data\n",
    "#\n",
    "# Hint 1: Drop any \"CLASS\" and \"ID\" columns first and then apply imputation, normalization and (possibly) one-hot\n",
    "# Hint 2: Get the numerical values (as an ndarray) from the resulting dataframe and iterate over the rows \n",
    "#         calling some sub-function, e.g., get_nearest_neighbor_predictions(x_test,k), which for a test row\n",
    "#         (numerical input feature values) finds the k nearest neighbors and calculate the class probabilities.\n",
    "# Hint 3: This sub-function may first find the distances to all training instances, e.g., pairs consisting of\n",
    "#         training instance index and distance, and then sort them according to distance, and then (using the indexes\n",
    "#         of the k closest instances) find the corresponding labels and calculate the relative class frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN:\n",
    "    def __init__(self):\n",
    "        self.imputation = None\n",
    "        self.normalization = None\n",
    "        self.one_hot = None\n",
    "        self.labels = None\n",
    "        self.training_labels = None\n",
    "        self.training_data = None\n",
    "        \n",
    "    def fit(self, df_init, normalizationtype = 'minmax'):\n",
    "        df = df_init.copy()\n",
    "        df, self.imputation = create_imputation(df)\n",
    "        df, self.normalization = create_normalization(df, normalizationtype)\n",
    "        df, self.one_hot = create_one_hot(df)\n",
    "        classColumn = df.loc[:,'CLASS'].astype('category')\n",
    "        self.training_labels = classColumn\n",
    "        self.labels = classColumn.cat.categories\n",
    "        df = df.drop(labels = ['CLASS', 'ID'], axis = 1)\n",
    "        self.training_data = df\n",
    "        \n",
    "    def getKNearestNeighbors(self, v1, k):\n",
    "        df = self.training_data\n",
    "        mat = df.values\n",
    "        nrow = df.shape[0]\n",
    "        matVect = np.tile(v1, nrow).reshape(nrow,len(v1))\n",
    "        dist = np.sum((mat-matVect)**2, axis = 1)\n",
    "        kNearestIndexes = dist.argsort()[:k]\n",
    "        return(kNearestIndexes)\n",
    "    \n",
    "    def getProbClass(self, kNearestIndexes):\n",
    "        k = kNearestIndexes.shape[0]\n",
    "        labelsData, labels = self.training_labels, self.labels\n",
    "        labelsProb = np.zeros(len(labels))\n",
    "        for i in range(k):\n",
    "            labelVect = np.array((labels == labelsData[kNearestIndexes[i]]), dtype='int64')\n",
    "            labelsProb += labelVect\n",
    "        labelsProb = labelsProb / np.sum(labelsProb)\n",
    "        return(labelsProb)\n",
    "    \n",
    "    def predict(self, df_init, k = 5):\n",
    "        df = df_init.copy()\n",
    "        nrow = df.shape[0]\n",
    "        probs = pd.DataFrame(columns = self.labels)\n",
    "        df = df.drop(['CLASS', 'ID'], axis = 1)\n",
    "        df = apply_imputation(df, self.imputation)\n",
    "        df = apply_normalization(df, self.normalization)\n",
    "        df = apply_one_hot(df, self.one_hot)\n",
    "        matrix = np.array([np.zeros(len(self.labels)) for i in range (nrow)])\n",
    "        for i in range(nrow):\n",
    "            v1 = df.iloc[i,:].values\n",
    "            kNearestIndexes = self.getKNearestNeighbors(v1 = v1, k = k)\n",
    "            labelsProb = self.getProbClass(kNearestIndexes = kNearestIndexes)\n",
    "            matrix[i] = labelsProb\n",
    "        probs = pd.DataFrame(matrix, columns=self.labels)\n",
    "        return(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.02 s.\n",
      "Testing time (k=1): 0.06 s.\n",
      "Testing time (k=3): 0.06 s.\n",
      "Testing time (k=5): 0.08 s.\n",
      "Testing time (k=7): 0.10 s.\n",
      "Testing time (k=9): 0.12 s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Brier score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>0.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.663551</td>\n",
       "      <td>0.488058</td>\n",
       "      <td>0.813829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.579439</td>\n",
       "      <td>0.471028</td>\n",
       "      <td>0.833843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.471867</td>\n",
       "      <td>0.833481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.482981</td>\n",
       "      <td>0.827727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Brier score       AUC\n",
       "1  0.747664     0.504673  0.758200\n",
       "3  0.663551     0.488058  0.813829\n",
       "5  0.579439     0.471028  0.833843\n",
       "7  0.598131     0.471867  0.833481\n",
       "9  0.616822     0.482981  0.827727"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your code (leave this part unchanged, except for if auc is undefined)\n",
    "\n",
    "glass_train_df = pd.read_csv(\"glass_train.txt\")\n",
    "\n",
    "glass_test_df = pd.read_csv(\"glass_test.txt\")\n",
    "\n",
    "knn_model = kNN()\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "knn_model.fit(glass_train_df)\n",
    "print(\"Training time: {0:.2f} s.\".format(time.perf_counter()-t0))\n",
    "\n",
    "test_labels = glass_test_df[\"CLASS\"]\n",
    "\n",
    "k_values = [1,3,5,7,9]\n",
    "results = np.empty((len(k_values),3))\n",
    "\n",
    "for i in range(len(k_values)):\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = knn_model.predict(glass_test_df,k=k_values[i])\n",
    "    print(\"Testing time (k={0}): {1:.2f} s.\".format(k_values[i],time.perf_counter()-t0))\n",
    "    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n",
    "                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n",
    "\n",
    "results = pd.DataFrame(results,index=k_values,columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set (k=1): 1.00\n",
      "AUC on training set (k=1): 1.00\n",
      "Brier score on training set (k=1): 0.00\n"
     ]
    }
   ],
   "source": [
    "train_labels = glass_train_df[\"CLASS\"]\n",
    "predictions = knn_model.predict(glass_train_df,k=1)\n",
    "print(\"Accuracy on training set (k=1): {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
    "print(\"AUC on training set (k=1): {0:.2f}\".format(auc(predictions,train_labels)))\n",
    "print(\"Brier score on training set (k=1): {0:.2f}\".format(brier_score(predictions,train_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.02 s.\n",
      "Testing time (k=1): 0.15 s.\n",
      "Testing time (k=3): 0.14 s.\n",
      "Testing time (k=5): 0.14 s.\n",
      "Testing time (k=7): 0.14 s.\n",
      "Testing time (k=9): 0.14 s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Brier score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>0.810350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.663551</td>\n",
       "      <td>0.488058</td>\n",
       "      <td>0.815859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.579439</td>\n",
       "      <td>0.471028</td>\n",
       "      <td>0.833843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.471867</td>\n",
       "      <td>0.833481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.482981</td>\n",
       "      <td>0.827727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Brier score       AUC\n",
       "1  0.747664     0.504673  0.810350\n",
       "3  0.663551     0.488058  0.815859\n",
       "5  0.579439     0.471028  0.833843\n",
       "7  0.598131     0.471867  0.833481\n",
       "9  0.616822     0.482981  0.827727"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your code (leave this part unchanged, except for if auc is undefined)\n",
    "\n",
    "glass_train_df = pd.read_csv(\"glass_train.txt\")\n",
    "\n",
    "glass_test_df = pd.read_csv(\"glass_test.txt\")\n",
    "\n",
    "knn_model = kNN()\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "knn_model.fit(glass_train_df)\n",
    "print(\"Training time: {0:.2f} s.\".format(time.perf_counter()-t0))\n",
    "\n",
    "test_labels = glass_test_df[\"CLASS\"]\n",
    "\n",
    "k_values = [1,3,5,7,9]\n",
    "results = np.empty((len(k_values),3))\n",
    "\n",
    "for i in range(len(k_values)):\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = knn_model.predict(glass_test_df,k=k_values[i])\n",
    "    print(\"Testing time (k={0}): {1:.2f} s.\".format(k_values[i],time.perf_counter()-t0))\n",
    "    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n",
    "                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n",
    "\n",
    "results = pd.DataFrame(results,index=k_values,columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set (k=1): 1.00\n",
      "AUC on training set (k=1): 1.00\n",
      "Brier score on training set (k=1): 0.00\n"
     ]
    }
   ],
   "source": [
    "train_labels = glass_train_df[\"CLASS\"]\n",
    "predictions = knn_model.predict(glass_train_df,k=1)\n",
    "print(\"Accuracy on training set (k=1): {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
    "print(\"AUC on training set (k=1): {0:.2f}\".format(auc(predictions,train_labels)))\n",
    "print(\"Brier score on training set (k=1): {0:.2f}\".format(brier_score(predictions,train_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on assumptions, things that do not work properly, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the class NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class NaiveBayes with three functions __init__, fit and predict (after the comments):\n",
    "#\n",
    "# Input to __init__: \n",
    "# self: the object itself\n",
    "#\n",
    "# Output from __init__:\n",
    "# nothing\n",
    "# \n",
    "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
    "# binning, class_priors, feature_class_value_counts, feature_class_counts\n",
    "#\n",
    "# Input to fit:\n",
    "# self: the object itself\n",
    "# df: a dataframe (where the column names \"CLASS\" and \"ID\" have special meaning)\n",
    "# nobins: no. of bins (default = 10)\n",
    "# bintype: either \"equal-width\" (default) or \"equal-size\" \n",
    "#\n",
    "# Output from fit:\n",
    "# nothing\n",
    "#\n",
    "# The result of applying this function should be:\n",
    "#\n",
    "# self.binning should be a discretization mapping (see Assignment 1) from df\n",
    "# self.class_priors should be a mapping (dictionary) from the labels (categories) of the \"CLASS\" column of df,\n",
    "# to the relative frequencies of the labels\n",
    "# self.feature_class_value_counts should be a mapping from the feature (column name) to the number of\n",
    "# training instances with a specific combination of (non-missing, categorical) value for the feature and class label\n",
    "# self.feature_class_counts should me a mapping from the feature (column name) to the number of\n",
    "# training instances with a specific class label and some (non-missing, categorical) value for the feature\n",
    "# Note that the function does not return anything but just assigns values to the attributes of the object.\n",
    "#\n",
    "# Input to predict:\n",
    "# self: the object itself\n",
    "# df: a dataframe\n",
    "# \n",
    "# Output from predict:\n",
    "# predictions: a dataframe with class labels as column names and the rows corresponding to\n",
    "# predictions with estimated class probabilities for each row in df, where the class probabilities\n",
    "# are estimated by the naive approximation of Bayes rule (see lecture slides)\n",
    "#\n",
    "# Hint 1: First apply discretization\n",
    "# Hint 2: Iterating over either columns or rows, and for each possible class label, calculate the relative\n",
    "#         frequency of the observed feature value given the class (using feature_class_value_counts and \n",
    "#         feature_class_counts) \n",
    "# Hint 3: Calculate the non-normalized estimated class probabilities by multiplying the class priors to the\n",
    "#         product of the relative frequencies\n",
    "# Hint 4: Normalize the probabilities by dividing by the sum of the non-normalized probabilities; in case\n",
    "#         this sum is zero, then set the probabilities to the class priors\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define the class NaiveBayes with three functions __init__, fit and predict (after the comments):\n",
    "#\n",
    "# Input to __init__: \n",
    "# self: the object itself\n",
    "#\n",
    "# Output from __init__:\n",
    "# nothing\n",
    "# \n",
    "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
    "# binning, class_priors, feature_class_value_counts, feature_class_counts\n",
    "#\n",
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.binning = None\n",
    "        self.class_priors = None\n",
    "        self.feature_class_value_counts = None\n",
    "        self.feature_class_counts = None\n",
    "        self.labels = None\n",
    "        self.tot = None\n",
    "        \n",
    "    def fit(self, df_init, nobins = 10, bintype = 'equal-width'):\n",
    "        df = df_init.copy()\n",
    "        df, self.binning = create_bins(df, nobins, bintype)\n",
    "        classColumn = df.loc[:, 'CLASS'].astype('category')\n",
    "        self.class_priors = dict(classColumn.value_counts(normalize = True))\n",
    "        self.labels = classColumn.cat.categories\n",
    "        self.tot = df.shape[0]\n",
    "        dictCount, dictValueCount = {}, {}\n",
    "        for col in df.columns:\n",
    "            if(col not in ['CLASS', 'ID']):\n",
    "                df2 = df.dropna(axis = 0, how='any', subset = ['CLASS', col])\n",
    "                dfg = df2.groupby(['CLASS', col]).size()\n",
    "                dictCount[col] = dict(df2.loc[:,col].value_counts(normalize = True))\n",
    "                dictValueCount[col] = dict(dfg)\n",
    "        self.feature_class_counts = dictCount\n",
    "        self.feature_class_value_counts = dictValueCount\n",
    "        \n",
    "    def predict(self, df_init):\n",
    "        df = df_init.copy()\n",
    "        df = apply_bins(df, self.binning)\n",
    "        df = df.drop(labels = ['CLASS', 'ID'], axis = 1)\n",
    "        labels = self.labels\n",
    "        nrow, ncol, nlabel = df.shape[0], df.shape[1], len(labels)\n",
    "        matrix = np.zeros([nlabel, nrow, ncol])\n",
    "\n",
    "        \n",
    "        for col_num in range(ncol):\n",
    "            col = df.columns[col_num]\n",
    "            \n",
    "            for label_num in range(nlabel):\n",
    "                label = labels[label_num]\n",
    "\n",
    "                for row_num in range(nrow):\n",
    "                    value = df.iloc[row_num, col_num]\n",
    "                    if((label, value) in self.feature_class_value_counts[col].keys()):\n",
    "                        features_value_count = self.feature_class_value_counts[col][(label, value)]\n",
    "                        feature_count = self.feature_class_counts[col][value]\n",
    "                        relat_frequency = features_value_count / feature_count\n",
    "                    else:\n",
    "                        relat_frequency = 0\n",
    "                    matrix[label_num, row_num, col_num] = relat_frequency\n",
    "        \n",
    "        \n",
    "        #matProduct = matrix.prod(axis = 2)\n",
    "        #matrix = matrix / self.tot\n",
    "        mat_non_normalized = matrix.prod(axis = 2)\n",
    "        classesVect = np.array([self.class_priors[i] for i in labels])\n",
    "        matClasses = np.tile(classesVect, nrow).reshape([nrow, nlabel]).T\n",
    "        #mat_non_normalized = (matProduct * matClasses)\n",
    "        normalization = np.sum(mat_non_normalized, axis = 0)\n",
    "        #print(mat_non_normalized)\n",
    "        normalizing_mat = np.tile(normalization, nlabel).reshape([nlabel, nrow])\n",
    "        normalizing_mat_zero = normalizing_mat==0\n",
    "        normalizing_mat_zero = normalizing_mat_zero.astype('float')*matClasses \n",
    "        normalizing_mat += normalizing_mat_zero\n",
    "        normalized_mat = mat_non_normalized / normalizing_mat\n",
    "        result = pd.DataFrame(normalized_mat.T, columns = list(self.class_priors.keys()))\n",
    "        cols = result.columns.tolist()\n",
    "        cols = sorted(cols)\n",
    "        result = result[cols]\n",
    "        print(result)\n",
    "\n",
    "        #result = pd.DataFrame(matProduct.T, columns = list(self.class_priors.keys()))\n",
    "        return(result)\n",
    "\n",
    "# Input to predict:\n",
    "# self: the object itself\n",
    "# df: a dataframe\n",
    "# \n",
    "# Output from predict:\n",
    "# predictions: a dataframe with class labels as column names and the rows corresponding to\n",
    "# predictions with estimated class probabilities for each row in df, where the class probabilities\n",
    "# are estimated by the naive approximation of Bayes rule (see lecture slides)\n",
    "#\n",
    "# Hint 1: First apply discretization\n",
    "# Hint 2: Iterating over either columns or rows, and for each possible class label, calculate the relative\n",
    "#         frequency of the observed feature value given the class (using feature_class_value_counts and \n",
    "#         feature_class_counts) \n",
    "# Hint 3: Calculate the non-normalized estimated class probabilities by multiplying the class priors to the\n",
    "#         product of the relative frequencies\n",
    "# Hint 4: Normalize the probabilities by dividing by the sum of the non-normalized probabilities; in case\n",
    "#         this sum is zero, then set the probabilities to the class priors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class NaiveBayes with three functions __init__, fit and predict (after the comments):\n",
    "#\n",
    "# Input to __init__: \n",
    "# self: the object itself\n",
    "#\n",
    "# Output from __init__:\n",
    "# nothing\n",
    "# \n",
    "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
    "# binning, class_priors, feature_class_value_counts, feature_class_counts\n",
    "#\n",
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.binning = None\n",
    "        self.class_priors = None\n",
    "        self.feature_class_value_counts = None\n",
    "        self.feature_class_counts = None\n",
    "        self.labels = None\n",
    "        self.tot = None\n",
    "        \n",
    "    def fit(self, df_init, nobins = 10, bintype = 'equal-width'):\n",
    "        df = df_init.copy()\n",
    "        df, self.binning = create_bins(df, nobins, bintype)\n",
    "        #display(HTML(df.to_html()))\n",
    "        classColumn = df.loc[:, 'CLASS'].astype('category')\n",
    "        self.class_priors = dict(classColumn.value_counts(normalize = True))\n",
    "        self.labels = classColumn.cat.categories\n",
    "        self.tot = df.shape[0]\n",
    "        dictCount, dictValueCount = {}, {}\n",
    "        for col in df.columns:\n",
    "            if(col not in ['CLASS', 'ID']):\n",
    "                df2 = df.dropna(axis = 0, how='any', subset = ['CLASS', col])\n",
    "                dfg = df2.groupby(['CLASS', col]).size()\n",
    "                dictCount[col] = dict(df2.loc[:,'CLASS'].value_counts())\n",
    "                #if(col == 'RI'):\n",
    "                    #print(dfg)\n",
    "                dictValueCount[col] = dict(dfg)\n",
    "        self.feature_class_counts = dictCount\n",
    "        self.feature_class_value_counts = dictValueCount\n",
    "        \n",
    "    def predict(self, df_init):\n",
    "        df = df_init.copy()\n",
    "        df = apply_bins(df, self.binning)\n",
    "\n",
    "        df = df.drop(labels = ['CLASS', 'ID'], axis = 1)\n",
    "        labels = self.labels\n",
    "        nrow, ncol, nlabel = df.shape[0], df.shape[1], len(labels)\n",
    "        matrix = np.zeros([nlabel+2, nrow, ncol])\n",
    "        \n",
    "        for col_num in range(ncol):\n",
    "            col = df.columns[col_num]\n",
    "            \n",
    "            for label_num in range(nlabel):\n",
    "                label = labels[label_num]\n",
    "\n",
    "                for row_num in range(nrow):\n",
    "                    value = df.iloc[row_num, col_num]\n",
    "                    if((label, value) in self.feature_class_value_counts[col].keys()):\n",
    "                        features_value_count = self.feature_class_value_counts[col][(label, value)]\n",
    "                        feature_count = self.feature_class_counts[col][label]\n",
    "                        relat_frequency = features_value_count / feature_count\n",
    "                    else:\n",
    "                        relat_frequency = 0\n",
    "                    \n",
    "                    matrix[label, row_num, col_num] = relat_frequency\n",
    "        \n",
    "        # We multiply the relative frequencies of the feature between eachother for a specific (rowNumber, classLabel)\n",
    "        mat_non_normalized = matrix.prod(axis = 2)\n",
    "        \n",
    "        classesVect = np.array([self.class_priors[i] if i in self.class_priors.keys() else 0 for i in range(0,8)])\n",
    "        \n",
    "        matClasses = np.tile(classesVect, nrow).reshape([nrow, nlabel+2]).T\n",
    "        \n",
    "        mat_non_normalized = (mat_non_normalized * matClasses)\n",
    "        normalization = np.sum(mat_non_normalized, axis = 0)\n",
    "        \n",
    "        normalizing_mat = np.tile(normalization, nlabel+2).reshape([nlabel+2, nrow])\n",
    "        normalizing_mat_zero = normalizing_mat==0\n",
    "        normalizing_mat_zero = normalizing_mat_zero.astype('float')*matClasses \n",
    "        normalizing_mat += normalizing_mat_zero\n",
    "        normalized_mat = mat_non_normalized / normalizing_mat\n",
    "        result = pd.DataFrame(normalized_mat[1:,:].T, columns = [i for i in range(1,8)])\n",
    "        colsToKeep = (np.sum(result.values, axis = 0)>0)\n",
    "        result = result.loc[:, colsToKeep]\n",
    "        print(result)\n",
    "        #result = pd.DataFrame(matProduct.T, columns = list(self.class_priors.keys()))\n",
    "        return(result)\n",
    "\n",
    "# Input to predict:\n",
    "# self: the object itself\n",
    "# df: a dataframe\n",
    "# \n",
    "# Output from predict:\n",
    "# predictions: a dataframe with class labels as column names and the rows corresponding to\n",
    "# predictions with estimated class probabilities for each row in df, where the class probabilities\n",
    "# are estimated by the naive approximation of Bayes rule (see lecture slides)\n",
    "#\n",
    "# Hint 1: First apply discretization\n",
    "# Hint 2: Iterating over either columns or rows, and for each possible class label, calculate the relative\n",
    "#         frequency of the observed feature value given the class (using feature_class_value_counts and \n",
    "#         feature_class_counts) \n",
    "# Hint 3: Calculate the non-normalized estimated class probabilities by multiplying the class priors to the\n",
    "#         product of the relative frequencies\n",
    "# Hint 4: Normalize the probabilities by dividing by the sum of the non-normalized probabilities; in case\n",
    "#         this sum is zero, then set the probabilities to the class priors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (3, 'equal-width'): 0.13 s.\n",
      "            1         2         3         5         6         7\n",
      "0    0.788983  0.211017  0.000000  0.000000  0.000000  0.000000\n",
      "1    0.000000  1.000000  0.000000  0.000000  0.000000  0.000000\n",
      "2    0.743779  0.054675  0.201289  0.000000  0.000000  0.000256\n",
      "3    0.777818  0.088274  0.132948  0.000000  0.000000  0.000960\n",
      "4    0.239973  0.732356  0.024903  0.000000  0.000000  0.002768\n",
      "5    0.855868  0.144132  0.000000  0.000000  0.000000  0.000000\n",
      "6    0.239973  0.732356  0.024903  0.000000  0.000000  0.002768\n",
      "7    0.239973  0.732356  0.024903  0.000000  0.000000  0.002768\n",
      "8    0.734157  0.206060  0.059606  0.000000  0.000000  0.000177\n",
      "9    0.000000  0.071892  0.000000  0.000000  0.000000  0.928108\n",
      "10   0.000000  0.000000  0.000000  0.000000  0.000000  1.000000\n",
      "11   0.000000  0.000000  0.000000  0.000000  0.000000  1.000000\n",
      "12   0.239973  0.732356  0.024903  0.000000  0.000000  0.002768\n",
      "13   0.743779  0.054675  0.201289  0.000000  0.000000  0.000256\n",
      "14   0.777818  0.088274  0.132948  0.000000  0.000000  0.000960\n",
      "15   0.777818  0.088274  0.132948  0.000000  0.000000  0.000960\n",
      "16   0.525476  0.025752  0.442431  0.000000  0.000000  0.006341\n",
      "17   0.239973  0.732356  0.024903  0.000000  0.000000  0.002768\n",
      "18   0.239973  0.732356  0.024903  0.000000  0.000000  0.002768\n",
      "19   0.000000  0.000000  0.000000  0.000000  0.000000  1.000000\n",
      "20   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "21   0.457446  0.542554  0.000000  0.000000  0.000000  0.000000\n",
      "22   0.852342  0.147658  0.000000  0.000000  0.000000  0.000000\n",
      "23   0.734157  0.206060  0.059606  0.000000  0.000000  0.000177\n",
      "24   0.221150  0.755900  0.022950  0.000000  0.000000  0.000000\n",
      "25   0.673190  0.291709  0.034519  0.000000  0.000000  0.000581\n",
      "26   0.675870  0.324130  0.000000  0.000000  0.000000  0.000000\n",
      "27   0.957757  0.042243  0.000000  0.000000  0.000000  0.000000\n",
      "28   0.462601  0.369752  0.160023  0.000000  0.000000  0.007624\n",
      "29   0.296009  0.655353  0.048638  0.000000  0.000000  0.000000\n",
      "..        ...       ...       ...       ...       ...       ...\n",
      "77   0.957757  0.042243  0.000000  0.000000  0.000000  0.000000\n",
      "78   0.852342  0.147658  0.000000  0.000000  0.000000  0.000000\n",
      "79   0.000000  1.000000  0.000000  0.000000  0.000000  0.000000\n",
      "80   0.000000  0.060460  0.000000  0.493175  0.380020  0.066344\n",
      "81   0.353220  0.646780  0.000000  0.000000  0.000000  0.000000\n",
      "82   0.743779  0.054675  0.201289  0.000000  0.000000  0.000256\n",
      "83   0.239973  0.732356  0.024903  0.000000  0.000000  0.002768\n",
      "84   0.462601  0.369752  0.160023  0.000000  0.000000  0.007624\n",
      "85   0.743779  0.054675  0.201289  0.000000  0.000000  0.000256\n",
      "86   0.462601  0.369752  0.160023  0.000000  0.000000  0.007624\n",
      "87   0.000000  1.000000  0.000000  0.000000  0.000000  0.000000\n",
      "88   0.147860  0.042691  0.000000  0.188721  0.605918  0.014809\n",
      "89   0.000000  0.000000  0.000000  0.000000  0.000000  1.000000\n",
      "90   0.318037  0.628681  0.052257  0.000000  0.000000  0.001025\n",
      "91   0.734157  0.206060  0.059606  0.000000  0.000000  0.000177\n",
      "92   0.000000  0.341342  0.000000  0.658658  0.000000  0.000000\n",
      "93   0.743779  0.054675  0.201289  0.000000  0.000000  0.000256\n",
      "94   0.318037  0.628681  0.052257  0.000000  0.000000  0.001025\n",
      "95   0.483084  0.250103  0.264588  0.000000  0.000000  0.002225\n",
      "96   0.239973  0.732356  0.024903  0.000000  0.000000  0.002768\n",
      "97   0.957757  0.042243  0.000000  0.000000  0.000000  0.000000\n",
      "98   0.462601  0.369752  0.160023  0.000000  0.000000  0.007624\n",
      "99   0.000000  0.000000  0.000000  0.000000  0.000000  1.000000\n",
      "100  0.743779  0.054675  0.201289  0.000000  0.000000  0.000256\n",
      "101  0.462601  0.369752  0.160023  0.000000  0.000000  0.007624\n",
      "102  0.239973  0.732356  0.024903  0.000000  0.000000  0.002768\n",
      "103  0.239973  0.732356  0.024903  0.000000  0.000000  0.002768\n",
      "104  0.353220  0.646780  0.000000  0.000000  0.000000  0.000000\n",
      "105  0.000000  1.000000  0.000000  0.000000  0.000000  0.000000\n",
      "106  0.000000  0.202467  0.000000  0.512603  0.000000  0.284929\n",
      "\n",
      "[107 rows x 6 columns]\n",
      "Testing time (3, 'equal-width'): 0.19 s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:86: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Brier score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>equal-width</th>\n",
       "      <td>6.168224e-01</td>\n",
       "      <td>6.213252e-01</td>\n",
       "      <td>7.222657e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal-size</th>\n",
       "      <td>1.691100e-306</td>\n",
       "      <td>9.346037e-307</td>\n",
       "      <td>1.424198e-306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>equal-width</th>\n",
       "      <td>1.780191e-306</td>\n",
       "      <td>4.450615e-308</td>\n",
       "      <td>1.246121e-306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal-size</th>\n",
       "      <td>1.379620e-306</td>\n",
       "      <td>9.345976e-307</td>\n",
       "      <td>1.290618e-306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10</th>\n",
       "      <th>equal-width</th>\n",
       "      <td>1.780196e-306</td>\n",
       "      <td>1.112559e-306</td>\n",
       "      <td>8.900981e-307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal-size</th>\n",
       "      <td>9.346098e-307</td>\n",
       "      <td>3.917923e-317</td>\n",
       "      <td>8.344519e-308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy    Brier score            AUC\n",
       "3  equal-width   6.168224e-01   6.213252e-01   7.222657e-01\n",
       "   equal-size   1.691100e-306  9.346037e-307  1.424198e-306\n",
       "5  equal-width  1.780191e-306  4.450615e-308  1.246121e-306\n",
       "   equal-size   1.379620e-306  9.345976e-307  1.290618e-306\n",
       "10 equal-width  1.780196e-306  1.112559e-306  8.900981e-307\n",
       "   equal-size   9.346098e-307  3.917923e-317  8.344519e-308"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your code (leave this part unchanged, except for if auc is undefined)\n",
    "\n",
    "glass_train_df = pd.read_csv(\"glass_train.txt\")\n",
    "\n",
    "glass_test_df = pd.read_csv(\"glass_test.txt\")\n",
    "\n",
    "nb_model = NaiveBayes()\n",
    "\n",
    "test_labels = glass_test_df[\"CLASS\"]\n",
    "\n",
    "nobins_values = [3,5,10]\n",
    "bintype_values = [\"equal-width\",\"equal-size\"]\n",
    "parameters = [(nobins,bintype) for nobins in nobins_values for bintype in bintype_values]\n",
    "\n",
    "results = np.empty((len(parameters),3))\n",
    "\n",
    "for i in range(1):\n",
    "    t0 = time.perf_counter()\n",
    "    nb_model.fit(glass_train_df,nobins=parameters[i][0],bintype=parameters[i][1])\n",
    "    print(\"Training time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = nb_model.predict(glass_test_df)\n",
    "    print(\"Testing time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n",
    "                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n",
    "\n",
    "results = pd.DataFrame(results,index=pd.MultiIndex.from_product([nobins_values,bintype_values]),\n",
    "                       columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>1.51655</td>\n",
       "      <td>12.75</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.44</td>\n",
       "      <td>73.27</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.79</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>1.52725</td>\n",
       "      <td>13.80</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.66</td>\n",
       "      <td>70.57</td>\n",
       "      <td>0.08</td>\n",
       "      <td>11.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>1.52210</td>\n",
       "      <td>13.73</td>\n",
       "      <td>3.84</td>\n",
       "      <td>0.72</td>\n",
       "      <td>71.76</td>\n",
       "      <td>0.17</td>\n",
       "      <td>9.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>1.51784</td>\n",
       "      <td>12.68</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1.16</td>\n",
       "      <td>73.11</td>\n",
       "      <td>0.61</td>\n",
       "      <td>8.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81</td>\n",
       "      <td>1.51592</td>\n",
       "      <td>12.86</td>\n",
       "      <td>3.52</td>\n",
       "      <td>2.12</td>\n",
       "      <td>72.66</td>\n",
       "      <td>0.69</td>\n",
       "      <td>7.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>142</td>\n",
       "      <td>1.51851</td>\n",
       "      <td>13.20</td>\n",
       "      <td>3.63</td>\n",
       "      <td>1.07</td>\n",
       "      <td>72.83</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.41</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>120</td>\n",
       "      <td>1.51652</td>\n",
       "      <td>13.56</td>\n",
       "      <td>3.57</td>\n",
       "      <td>1.47</td>\n",
       "      <td>72.45</td>\n",
       "      <td>0.64</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>123</td>\n",
       "      <td>1.51687</td>\n",
       "      <td>13.23</td>\n",
       "      <td>3.54</td>\n",
       "      <td>1.48</td>\n",
       "      <td>72.84</td>\n",
       "      <td>0.56</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>133</td>\n",
       "      <td>1.51813</td>\n",
       "      <td>13.43</td>\n",
       "      <td>3.98</td>\n",
       "      <td>1.18</td>\n",
       "      <td>72.49</td>\n",
       "      <td>0.58</td>\n",
       "      <td>8.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>185</td>\n",
       "      <td>1.51115</td>\n",
       "      <td>17.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>75.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>212</td>\n",
       "      <td>1.52065</td>\n",
       "      <td>14.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.02</td>\n",
       "      <td>73.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.44</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>199</td>\n",
       "      <td>1.51531</td>\n",
       "      <td>14.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.66</td>\n",
       "      <td>73.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9.08</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>83</td>\n",
       "      <td>1.51646</td>\n",
       "      <td>13.41</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.25</td>\n",
       "      <td>72.81</td>\n",
       "      <td>0.68</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>158</td>\n",
       "      <td>1.52121</td>\n",
       "      <td>14.03</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.58</td>\n",
       "      <td>71.79</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>42</td>\n",
       "      <td>1.51755</td>\n",
       "      <td>12.71</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1.20</td>\n",
       "      <td>73.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>8.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>153</td>\n",
       "      <td>1.51779</td>\n",
       "      <td>13.64</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>73.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>40</td>\n",
       "      <td>1.52213</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.47</td>\n",
       "      <td>71.77</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>90</td>\n",
       "      <td>1.51640</td>\n",
       "      <td>12.55</td>\n",
       "      <td>3.48</td>\n",
       "      <td>1.87</td>\n",
       "      <td>73.23</td>\n",
       "      <td>0.63</td>\n",
       "      <td>8.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>149</td>\n",
       "      <td>1.51670</td>\n",
       "      <td>13.24</td>\n",
       "      <td>3.57</td>\n",
       "      <td>1.38</td>\n",
       "      <td>72.70</td>\n",
       "      <td>0.56</td>\n",
       "      <td>8.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>194</td>\n",
       "      <td>1.51719</td>\n",
       "      <td>14.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>73.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.53</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.08</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>164</td>\n",
       "      <td>1.51514</td>\n",
       "      <td>14.01</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.50</td>\n",
       "      <td>69.89</td>\n",
       "      <td>1.68</td>\n",
       "      <td>5.87</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>134</td>\n",
       "      <td>1.51800</td>\n",
       "      <td>13.71</td>\n",
       "      <td>3.93</td>\n",
       "      <td>1.54</td>\n",
       "      <td>71.81</td>\n",
       "      <td>0.54</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>128</td>\n",
       "      <td>1.52081</td>\n",
       "      <td>13.78</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.43</td>\n",
       "      <td>71.99</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>137</td>\n",
       "      <td>1.51806</td>\n",
       "      <td>13.00</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.08</td>\n",
       "      <td>73.07</td>\n",
       "      <td>0.56</td>\n",
       "      <td>8.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>136</td>\n",
       "      <td>1.51789</td>\n",
       "      <td>13.19</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.30</td>\n",
       "      <td>72.33</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>1.51756</td>\n",
       "      <td>13.15</td>\n",
       "      <td>3.61</td>\n",
       "      <td>1.05</td>\n",
       "      <td>73.24</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33</td>\n",
       "      <td>1.51775</td>\n",
       "      <td>12.85</td>\n",
       "      <td>3.48</td>\n",
       "      <td>1.23</td>\n",
       "      <td>72.97</td>\n",
       "      <td>0.61</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>162</td>\n",
       "      <td>1.51934</td>\n",
       "      <td>13.64</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.75</td>\n",
       "      <td>72.65</td>\n",
       "      <td>0.16</td>\n",
       "      <td>8.89</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>157</td>\n",
       "      <td>1.51655</td>\n",
       "      <td>13.41</td>\n",
       "      <td>3.39</td>\n",
       "      <td>1.28</td>\n",
       "      <td>72.64</td>\n",
       "      <td>0.52</td>\n",
       "      <td>8.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>72</td>\n",
       "      <td>1.51848</td>\n",
       "      <td>13.64</td>\n",
       "      <td>3.87</td>\n",
       "      <td>1.27</td>\n",
       "      <td>71.96</td>\n",
       "      <td>0.54</td>\n",
       "      <td>8.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>155</td>\n",
       "      <td>1.51694</td>\n",
       "      <td>12.86</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1.31</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>8.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>1.51768</td>\n",
       "      <td>12.65</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1.30</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.61</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>58</td>\n",
       "      <td>1.51824</td>\n",
       "      <td>12.87</td>\n",
       "      <td>3.48</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.95</td>\n",
       "      <td>0.60</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>95</td>\n",
       "      <td>1.51629</td>\n",
       "      <td>12.71</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.49</td>\n",
       "      <td>73.28</td>\n",
       "      <td>0.67</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>121</td>\n",
       "      <td>1.51844</td>\n",
       "      <td>13.25</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1.32</td>\n",
       "      <td>72.40</td>\n",
       "      <td>0.58</td>\n",
       "      <td>8.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7</td>\n",
       "      <td>1.51743</td>\n",
       "      <td>13.30</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.14</td>\n",
       "      <td>73.09</td>\n",
       "      <td>0.58</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5</td>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>181</td>\n",
       "      <td>1.51299</td>\n",
       "      <td>14.40</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.54</td>\n",
       "      <td>74.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>48</td>\n",
       "      <td>1.52667</td>\n",
       "      <td>13.99</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.71</td>\n",
       "      <td>71.57</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>24</td>\n",
       "      <td>1.51751</td>\n",
       "      <td>12.81</td>\n",
       "      <td>3.57</td>\n",
       "      <td>1.35</td>\n",
       "      <td>73.02</td>\n",
       "      <td>0.62</td>\n",
       "      <td>8.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>75</td>\n",
       "      <td>1.51596</td>\n",
       "      <td>13.02</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1.54</td>\n",
       "      <td>73.11</td>\n",
       "      <td>0.72</td>\n",
       "      <td>7.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>87</td>\n",
       "      <td>1.51569</td>\n",
       "      <td>13.24</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.47</td>\n",
       "      <td>73.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>8.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>151</td>\n",
       "      <td>1.51665</td>\n",
       "      <td>13.14</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1.76</td>\n",
       "      <td>72.48</td>\n",
       "      <td>0.60</td>\n",
       "      <td>8.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>78</td>\n",
       "      <td>1.51627</td>\n",
       "      <td>13.00</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.83</td>\n",
       "      <td>0.61</td>\n",
       "      <td>8.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>200</td>\n",
       "      <td>1.51609</td>\n",
       "      <td>15.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.51</td>\n",
       "      <td>73.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8.83</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>15</td>\n",
       "      <td>1.51763</td>\n",
       "      <td>12.61</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.31</td>\n",
       "      <td>73.29</td>\n",
       "      <td>0.58</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>85</td>\n",
       "      <td>1.51409</td>\n",
       "      <td>14.25</td>\n",
       "      <td>3.09</td>\n",
       "      <td>2.08</td>\n",
       "      <td>72.28</td>\n",
       "      <td>1.10</td>\n",
       "      <td>7.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>70</td>\n",
       "      <td>1.52300</td>\n",
       "      <td>13.31</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.82</td>\n",
       "      <td>71.99</td>\n",
       "      <td>0.12</td>\n",
       "      <td>10.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>54</td>\n",
       "      <td>1.51837</td>\n",
       "      <td>13.14</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1.28</td>\n",
       "      <td>72.85</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>118</td>\n",
       "      <td>1.51708</td>\n",
       "      <td>13.72</td>\n",
       "      <td>3.68</td>\n",
       "      <td>1.81</td>\n",
       "      <td>72.06</td>\n",
       "      <td>0.64</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>46</td>\n",
       "      <td>1.51900</td>\n",
       "      <td>13.49</td>\n",
       "      <td>3.48</td>\n",
       "      <td>1.35</td>\n",
       "      <td>71.95</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>114</td>\n",
       "      <td>1.51892</td>\n",
       "      <td>13.46</td>\n",
       "      <td>3.83</td>\n",
       "      <td>1.26</td>\n",
       "      <td>72.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>93</td>\n",
       "      <td>1.51588</td>\n",
       "      <td>13.12</td>\n",
       "      <td>3.41</td>\n",
       "      <td>1.58</td>\n",
       "      <td>73.26</td>\n",
       "      <td>0.07</td>\n",
       "      <td>8.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>37</td>\n",
       "      <td>1.51909</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.53</td>\n",
       "      <td>1.32</td>\n",
       "      <td>71.81</td>\n",
       "      <td>0.51</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>113</td>\n",
       "      <td>1.52777</td>\n",
       "      <td>12.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>72.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>14.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>20</td>\n",
       "      <td>1.51735</td>\n",
       "      <td>13.02</td>\n",
       "      <td>3.54</td>\n",
       "      <td>1.69</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.54</td>\n",
       "      <td>8.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>105</td>\n",
       "      <td>1.52410</td>\n",
       "      <td>13.83</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.17</td>\n",
       "      <td>71.15</td>\n",
       "      <td>0.08</td>\n",
       "      <td>10.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>28</td>\n",
       "      <td>1.51721</td>\n",
       "      <td>12.87</td>\n",
       "      <td>3.48</td>\n",
       "      <td>1.33</td>\n",
       "      <td>73.04</td>\n",
       "      <td>0.56</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>52</td>\n",
       "      <td>1.51926</td>\n",
       "      <td>13.20</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.28</td>\n",
       "      <td>72.36</td>\n",
       "      <td>0.60</td>\n",
       "      <td>9.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4</td>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>198</td>\n",
       "      <td>1.51727</td>\n",
       "      <td>14.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.34</td>\n",
       "      <td>73.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.95</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>213</td>\n",
       "      <td>1.51651</td>\n",
       "      <td>14.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.94</td>\n",
       "      <td>73.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.48</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>30</td>\n",
       "      <td>1.51784</td>\n",
       "      <td>13.08</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.28</td>\n",
       "      <td>72.86</td>\n",
       "      <td>0.60</td>\n",
       "      <td>8.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>39</td>\n",
       "      <td>1.52213</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.47</td>\n",
       "      <td>71.77</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>125</td>\n",
       "      <td>1.52177</td>\n",
       "      <td>13.20</td>\n",
       "      <td>3.68</td>\n",
       "      <td>1.15</td>\n",
       "      <td>72.75</td>\n",
       "      <td>0.54</td>\n",
       "      <td>8.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>103</td>\n",
       "      <td>1.51820</td>\n",
       "      <td>12.62</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.83</td>\n",
       "      <td>73.81</td>\n",
       "      <td>0.35</td>\n",
       "      <td>9.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>49</td>\n",
       "      <td>1.52223</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.79</td>\n",
       "      <td>71.99</td>\n",
       "      <td>0.13</td>\n",
       "      <td>10.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>175</td>\n",
       "      <td>1.52058</td>\n",
       "      <td>12.85</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.17</td>\n",
       "      <td>72.18</td>\n",
       "      <td>0.76</td>\n",
       "      <td>9.70</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.51</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>180</td>\n",
       "      <td>1.51852</td>\n",
       "      <td>14.09</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.66</td>\n",
       "      <td>72.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>170</td>\n",
       "      <td>1.51994</td>\n",
       "      <td>13.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.76</td>\n",
       "      <td>73.03</td>\n",
       "      <td>0.47</td>\n",
       "      <td>11.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>143</td>\n",
       "      <td>1.51662</td>\n",
       "      <td>12.85</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1.44</td>\n",
       "      <td>73.01</td>\n",
       "      <td>0.68</td>\n",
       "      <td>8.23</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>84</td>\n",
       "      <td>1.51594</td>\n",
       "      <td>13.09</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.55</td>\n",
       "      <td>72.87</td>\n",
       "      <td>0.68</td>\n",
       "      <td>8.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>126</td>\n",
       "      <td>1.51872</td>\n",
       "      <td>12.93</td>\n",
       "      <td>3.66</td>\n",
       "      <td>1.56</td>\n",
       "      <td>72.51</td>\n",
       "      <td>0.58</td>\n",
       "      <td>8.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>56</td>\n",
       "      <td>1.51769</td>\n",
       "      <td>12.45</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1.29</td>\n",
       "      <td>73.70</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>27</td>\n",
       "      <td>1.51793</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.48</td>\n",
       "      <td>1.41</td>\n",
       "      <td>72.64</td>\n",
       "      <td>0.59</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>112</td>\n",
       "      <td>1.52739</td>\n",
       "      <td>11.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>150</td>\n",
       "      <td>1.51643</td>\n",
       "      <td>12.16</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.35</td>\n",
       "      <td>72.89</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>97</td>\n",
       "      <td>1.51841</td>\n",
       "      <td>13.02</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.06</td>\n",
       "      <td>72.34</td>\n",
       "      <td>0.64</td>\n",
       "      <td>9.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>129</td>\n",
       "      <td>1.52068</td>\n",
       "      <td>13.55</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.67</td>\n",
       "      <td>72.18</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>108</td>\n",
       "      <td>1.53393</td>\n",
       "      <td>12.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>70.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>16.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>168</td>\n",
       "      <td>1.51969</td>\n",
       "      <td>12.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.65</td>\n",
       "      <td>73.75</td>\n",
       "      <td>0.38</td>\n",
       "      <td>11.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>79</td>\n",
       "      <td>1.51613</td>\n",
       "      <td>13.92</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.25</td>\n",
       "      <td>72.88</td>\n",
       "      <td>0.37</td>\n",
       "      <td>7.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>63</td>\n",
       "      <td>1.52172</td>\n",
       "      <td>13.51</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.88</td>\n",
       "      <td>71.79</td>\n",
       "      <td>0.23</td>\n",
       "      <td>9.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>10</td>\n",
       "      <td>1.51755</td>\n",
       "      <td>13.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>35</td>\n",
       "      <td>1.51783</td>\n",
       "      <td>12.69</td>\n",
       "      <td>3.54</td>\n",
       "      <td>1.34</td>\n",
       "      <td>72.95</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>61</td>\n",
       "      <td>1.51905</td>\n",
       "      <td>13.60</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.11</td>\n",
       "      <td>72.64</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>29</td>\n",
       "      <td>1.51768</td>\n",
       "      <td>12.56</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.43</td>\n",
       "      <td>73.15</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>208</td>\n",
       "      <td>1.51831</td>\n",
       "      <td>14.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.82</td>\n",
       "      <td>72.86</td>\n",
       "      <td>1.41</td>\n",
       "      <td>6.47</td>\n",
       "      <td>2.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>100</td>\n",
       "      <td>1.51811</td>\n",
       "      <td>12.96</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.43</td>\n",
       "      <td>72.92</td>\n",
       "      <td>0.60</td>\n",
       "      <td>8.79</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>187</td>\n",
       "      <td>1.51838</td>\n",
       "      <td>14.32</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.22</td>\n",
       "      <td>71.25</td>\n",
       "      <td>1.46</td>\n",
       "      <td>5.79</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>135</td>\n",
       "      <td>1.51811</td>\n",
       "      <td>13.33</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.25</td>\n",
       "      <td>72.78</td>\n",
       "      <td>0.52</td>\n",
       "      <td>8.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>115</td>\n",
       "      <td>1.51847</td>\n",
       "      <td>13.10</td>\n",
       "      <td>3.97</td>\n",
       "      <td>1.19</td>\n",
       "      <td>72.44</td>\n",
       "      <td>0.60</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>171</td>\n",
       "      <td>1.52369</td>\n",
       "      <td>13.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.58</td>\n",
       "      <td>72.22</td>\n",
       "      <td>0.32</td>\n",
       "      <td>12.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>19</td>\n",
       "      <td>1.51911</td>\n",
       "      <td>13.90</td>\n",
       "      <td>3.73</td>\n",
       "      <td>1.18</td>\n",
       "      <td>72.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>9</td>\n",
       "      <td>1.51918</td>\n",
       "      <td>14.04</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1.37</td>\n",
       "      <td>72.08</td>\n",
       "      <td>0.56</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>38</td>\n",
       "      <td>1.51797</td>\n",
       "      <td>12.74</td>\n",
       "      <td>3.48</td>\n",
       "      <td>1.35</td>\n",
       "      <td>72.96</td>\n",
       "      <td>0.64</td>\n",
       "      <td>8.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>154</td>\n",
       "      <td>1.51610</td>\n",
       "      <td>13.42</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.22</td>\n",
       "      <td>72.69</td>\n",
       "      <td>0.59</td>\n",
       "      <td>8.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>69</td>\n",
       "      <td>1.52152</td>\n",
       "      <td>13.12</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.90</td>\n",
       "      <td>72.20</td>\n",
       "      <td>0.23</td>\n",
       "      <td>9.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>23</td>\n",
       "      <td>1.51736</td>\n",
       "      <td>12.78</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.79</td>\n",
       "      <td>0.59</td>\n",
       "      <td>8.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>196</td>\n",
       "      <td>1.51545</td>\n",
       "      <td>14.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.68</td>\n",
       "      <td>73.39</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9.07</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.05</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>66</td>\n",
       "      <td>1.52099</td>\n",
       "      <td>13.69</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.12</td>\n",
       "      <td>71.96</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>26</td>\n",
       "      <td>1.51764</td>\n",
       "      <td>12.98</td>\n",
       "      <td>3.54</td>\n",
       "      <td>1.21</td>\n",
       "      <td>73.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>8.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>80</td>\n",
       "      <td>1.51590</td>\n",
       "      <td>12.82</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.90</td>\n",
       "      <td>72.86</td>\n",
       "      <td>0.69</td>\n",
       "      <td>7.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>141</td>\n",
       "      <td>1.51690</td>\n",
       "      <td>13.33</td>\n",
       "      <td>3.54</td>\n",
       "      <td>1.61</td>\n",
       "      <td>72.54</td>\n",
       "      <td>0.68</td>\n",
       "      <td>8.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>14</td>\n",
       "      <td>1.51748</td>\n",
       "      <td>12.86</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1.27</td>\n",
       "      <td>73.21</td>\n",
       "      <td>0.54</td>\n",
       "      <td>8.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>132</td>\n",
       "      <td>1.52614</td>\n",
       "      <td>13.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.36</td>\n",
       "      <td>71.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>13.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>102</td>\n",
       "      <td>1.51730</td>\n",
       "      <td>12.35</td>\n",
       "      <td>2.72</td>\n",
       "      <td>1.63</td>\n",
       "      <td>72.87</td>\n",
       "      <td>0.70</td>\n",
       "      <td>9.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (3, 'equal-width'): 0.14 s.\n",
      "            1         2         3         5         6             7\n",
      "0    0.358974  0.641026  0.000000  0.000000  0.000000  0.000000e+00\n",
      "1    1.000000  0.000000  0.000000  0.000000  0.000000  0.000000e+00\n",
      "2    0.133384  0.866602  0.000000  0.000000  0.000009  4.613424e-06\n",
      "3    0.191995  0.807973  0.000000  0.000000  0.000030  2.716614e-06\n",
      "4    0.864641  0.135312  0.000000  0.000000  0.000047  2.762228e-07\n",
      "5    0.260688  0.739312  0.000000  0.000000  0.000000  0.000000e+00\n",
      "6    0.864641  0.135312  0.000000  0.000000  0.000047  2.762228e-07\n",
      "7    0.864641  0.135312  0.000000  0.000000  0.000047  2.762228e-07\n",
      "8    0.370148  0.629846  0.000000  0.000000  0.000005  1.005910e-06\n",
      "9    0.843836  0.000000  0.000000  0.000000  0.156164  0.000000e+00\n",
      "10   0.000000  0.000000  0.000000  0.000000  1.000000  0.000000e+00\n",
      "11   0.000000  0.000000  0.000000  0.000000  1.000000  0.000000e+00\n",
      "12   0.864641  0.135312  0.000000  0.000000  0.000047  2.762228e-07\n",
      "13   0.133384  0.866602  0.000000  0.000000  0.000009  4.613424e-06\n",
      "14   0.191995  0.807973  0.000000  0.000000  0.000030  2.716614e-06\n",
      "15   0.191995  0.807973  0.000000  0.000000  0.000030  2.716614e-06\n",
      "16   0.093030  0.906627  0.000000  0.000000  0.000328  1.501577e-05\n",
      "17   0.864641  0.135312  0.000000  0.000000  0.000047  2.762228e-07\n",
      "18   0.864641  0.135312  0.000000  0.000000  0.000047  2.762228e-07\n",
      "19   0.000000  0.000000  0.000000  0.000000  1.000000  0.000000e+00\n",
      "20   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000e+00\n",
      "21   0.712922  0.287078  0.000000  0.000000  0.000000  0.000000e+00\n",
      "22   0.266177  0.733823  0.000000  0.000000  0.000000  0.000000e+00\n",
      "23   0.370148  0.629846  0.000000  0.000000  0.000005  1.005910e-06\n",
      "24   0.877402  0.122598  0.000000  0.000000  0.000000  2.502675e-07\n",
      "25   0.475691  0.524295  0.000000  0.000000  0.000014  5.288446e-07\n",
      "26   0.501032  0.498968  0.000000  0.000000  0.000000  0.000000e+00\n",
      "27   0.084542  0.915458  0.000000  0.000000  0.000000  0.000000e+00\n",
      "28   0.625850  0.373963  0.000000  0.000000  0.000185  2.544657e-06\n",
      "29   0.822557  0.177442  0.000000  0.000000  0.000000  5.735246e-07\n",
      "..        ...       ...       ...       ...       ...           ...\n",
      "77   0.084542  0.915458  0.000000  0.000000  0.000000  0.000000e+00\n",
      "78   0.266177  0.733823  0.000000  0.000000  0.000000  0.000000e+00\n",
      "79   1.000000  0.000000  0.000000  0.000000  0.000000  0.000000e+00\n",
      "80   0.984433  0.000000  0.000006  0.000075  0.015486  0.000000e+00\n",
      "81   0.793131  0.206869  0.000000  0.000000  0.000000  0.000000e+00\n",
      "82   0.133384  0.866602  0.000000  0.000000  0.000009  4.613424e-06\n",
      "83   0.864641  0.135312  0.000000  0.000000  0.000047  2.762228e-07\n",
      "84   0.625850  0.373963  0.000000  0.000000  0.000185  2.544657e-06\n",
      "85   0.133384  0.866602  0.000000  0.000000  0.000009  4.613424e-06\n",
      "86   0.625850  0.373963  0.000000  0.000000  0.000185  2.544657e-06\n",
      "87   1.000000  0.000000  0.000000  0.000000  0.000000  0.000000e+00\n",
      "88   0.376058  0.622051  0.000005  0.000016  0.001870  0.000000e+00\n",
      "89   0.000000  0.000000  0.000000  0.000000  1.000000  0.000000e+00\n",
      "90   0.805392  0.194588  0.000000  0.000000  0.000019  6.289432e-07\n",
      "91   0.370148  0.629846  0.000000  0.000000  0.000005  1.005910e-06\n",
      "92   0.999982  0.000000  0.000000  0.000018  0.000000  0.000000e+00\n",
      "93   0.133384  0.866602  0.000000  0.000000  0.000009  4.613424e-06\n",
      "94   0.805392  0.194588  0.000000  0.000000  0.000019  6.289432e-07\n",
      "95   0.520119  0.479810  0.000000  0.000000  0.000066  5.169424e-06\n",
      "96   0.864641  0.135312  0.000000  0.000000  0.000047  2.762228e-07\n",
      "97   0.084542  0.915458  0.000000  0.000000  0.000000  0.000000e+00\n",
      "98   0.625850  0.373963  0.000000  0.000000  0.000185  2.544657e-06\n",
      "99   0.000000  0.000000  0.000000  0.000000  1.000000  0.000000e+00\n",
      "100  0.133384  0.866602  0.000000  0.000000  0.000009  4.613424e-06\n",
      "101  0.625850  0.373963  0.000000  0.000000  0.000185  2.544657e-06\n",
      "102  0.864641  0.135312  0.000000  0.000000  0.000047  2.762228e-07\n",
      "103  0.864641  0.135312  0.000000  0.000000  0.000047  2.762228e-07\n",
      "104  0.793131  0.206869  0.000000  0.000000  0.000000  0.000000e+00\n",
      "105  1.000000  0.000000  0.000000  0.000000  0.000000  0.000000e+00\n",
      "106  0.980202  0.000000  0.000000  0.000023  0.019775  0.000000e+00\n",
      "\n",
      "[107 rows x 6 columns]\n",
      "Testing time (3, 'equal-width'): 0.23 s.\n",
      "Training time (3, 'equal-size'): 0.11 s.\n",
      "            1         2         3         5         6         7\n",
      "0    0.795985  0.203989  0.000000  0.000000  0.000027  0.000000\n",
      "1    0.114601  0.884220  0.000043  0.000000  0.001129  0.000008\n",
      "2    0.117713  0.882281  0.000000  0.000000  0.000000  0.000006\n",
      "3    0.508847  0.491153  0.000000  0.000000  0.000000  0.000000\n",
      "4    0.992202  0.007700  0.000000  0.000000  0.000097  0.000000\n",
      "5    0.099076  0.900920  0.000000  0.000000  0.000000  0.000004\n",
      "6    0.955001  0.044999  0.000000  0.000000  0.000000  0.000000\n",
      "7    0.625059  0.374939  0.000000  0.000000  0.000000  0.000001\n",
      "8    0.374640  0.625354  0.000000  0.000000  0.000000  0.000006\n",
      "9    0.598674  0.000000  0.000000  0.000000  0.401326  0.000000\n",
      "10   0.077287  0.000000  0.000000  0.000000  0.922713  0.000000\n",
      "11   0.052386  0.000000  0.000000  0.000000  0.947614  0.000000\n",
      "12   0.765787  0.234213  0.000000  0.000000  0.000000  0.000000\n",
      "13   0.117713  0.882281  0.000000  0.000000  0.000000  0.000006\n",
      "14   0.249783  0.750204  0.000000  0.000000  0.000013  0.000000\n",
      "15   0.051021  0.948976  0.000000  0.000000  0.000000  0.000004\n",
      "16   0.117713  0.882281  0.000000  0.000000  0.000000  0.000006\n",
      "17   0.995818  0.003864  0.000000  0.000000  0.000318  0.000000\n",
      "18   0.287722  0.712271  0.000000  0.000000  0.000000  0.000008\n",
      "19   0.064636  0.000000  0.000000  0.000000  0.935364  0.000000\n",
      "20   0.986853  0.009300  0.000000  0.000000  0.003847  0.000000\n",
      "21   0.717998  0.281998  0.000000  0.000000  0.000000  0.000004\n",
      "22   0.493340  0.506620  0.000000  0.000000  0.000028  0.000013\n",
      "23   0.585836  0.414164  0.000000  0.000000  0.000000  0.000000\n",
      "24   0.381540  0.618437  0.000000  0.000000  0.000000  0.000023\n",
      "25   0.439623  0.560375  0.000000  0.000000  0.000000  0.000001\n",
      "26   0.334754  0.665234  0.000000  0.000000  0.000012  0.000000\n",
      "27   0.058563  0.941435  0.000000  0.000000  0.000000  0.000002\n",
      "28   0.519703  0.480274  0.000000  0.000000  0.000015  0.000007\n",
      "29   0.746571  0.253426  0.000000  0.000000  0.000000  0.000002\n",
      "..        ...       ...       ...       ...       ...       ...\n",
      "77   0.569376  0.430624  0.000000  0.000000  0.000000  0.000000\n",
      "78   0.998569  0.000000  0.000000  0.000068  0.001362  0.000000\n",
      "79   0.997378  0.000000  0.000000  0.000000  0.002622  0.000000\n",
      "80   0.997362  0.000000  0.000000  0.000368  0.002270  0.000000\n",
      "81   0.505256  0.494579  0.000000  0.000000  0.000164  0.000001\n",
      "82   0.195236  0.804750  0.000000  0.000000  0.000000  0.000014\n",
      "83   0.421408  0.578592  0.000000  0.000000  0.000000  0.000000\n",
      "84   0.421408  0.578592  0.000000  0.000000  0.000000  0.000000\n",
      "85   0.058563  0.941435  0.000000  0.000000  0.000000  0.000002\n",
      "86   0.585926  0.414067  0.000000  0.000000  0.000007  0.000000\n",
      "87   0.969166  0.000000  0.000000  0.000000  0.030834  0.000000\n",
      "88   0.681388  0.318606  0.000000  0.000000  0.000006  0.000000\n",
      "89   0.976916  0.021942  0.000000  0.000000  0.001143  0.000000\n",
      "90   0.281742  0.718257  0.000000  0.000000  0.000000  0.000002\n",
      "91   0.336733  0.663267  0.000000  0.000000  0.000000  0.000000\n",
      "92   0.998569  0.000000  0.000000  0.000068  0.001362  0.000000\n",
      "93   0.084556  0.915438  0.000000  0.000000  0.000000  0.000006\n",
      "94   0.741205  0.258793  0.000000  0.000000  0.000000  0.000002\n",
      "95   0.681388  0.318606  0.000000  0.000000  0.000006  0.000000\n",
      "96   0.512374  0.487616  0.000000  0.000000  0.000008  0.000002\n",
      "97   0.195236  0.804750  0.000000  0.000000  0.000000  0.000014\n",
      "98   0.421408  0.578592  0.000000  0.000000  0.000000  0.000000\n",
      "99   0.052386  0.000000  0.000000  0.000000  0.947614  0.000000\n",
      "100  0.120670  0.879322  0.000000  0.000000  0.000000  0.000008\n",
      "101  0.341244  0.658756  0.000000  0.000000  0.000000  0.000000\n",
      "102  0.992202  0.007700  0.000000  0.000000  0.000097  0.000000\n",
      "103  0.976045  0.023955  0.000000  0.000000  0.000000  0.000000\n",
      "104  0.857380  0.142620  0.000000  0.000000  0.000000  0.000000\n",
      "105  0.998218  0.000000  0.000000  0.000033  0.001749  0.000000\n",
      "106  0.942265  0.057615  0.000000  0.000000  0.000120  0.000000\n",
      "\n",
      "[107 rows x 6 columns]\n",
      "Testing time (3, 'equal-size'): 0.64 s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (5, 'equal-width'): 0.22 s.\n",
      "            1         2    3         5         6         7\n",
      "0    0.864741  0.135259  0.0  0.000000  0.000000  0.000000\n",
      "1    1.000000  0.000000  0.0  0.000000  0.000000  0.000000\n",
      "2    0.015225  0.984756  0.0  0.000000  0.000000  0.000019\n",
      "3    0.381382  0.618618  0.0  0.000000  0.000000  0.000000\n",
      "4    0.999486  0.000000  0.0  0.000000  0.000514  0.000000\n",
      "5    0.212744  0.787256  0.0  0.000000  0.000000  0.000000\n",
      "6    0.904837  0.095152  0.0  0.000000  0.000009  0.000001\n",
      "7    0.904837  0.095152  0.0  0.000000  0.000009  0.000001\n",
      "8    0.332354  0.667644  0.0  0.000000  0.000000  0.000002\n",
      "9    0.000000  0.000000  0.0  0.000000  0.000000  0.000000\n",
      "10   0.000000  0.000000  0.0  0.000000  1.000000  0.000000\n",
      "11   0.000000  0.000000  0.0  0.000000  1.000000  0.000000\n",
      "12   0.418277  0.581711  0.0  0.000000  0.000009  0.000003\n",
      "13   0.005441  0.994530  0.0  0.000000  0.000000  0.000030\n",
      "14   0.470894  0.528791  0.0  0.000000  0.000315  0.000000\n",
      "15   0.488258  0.511738  0.0  0.000000  0.000000  0.000004\n",
      "16   0.005441  0.994530  0.0  0.000000  0.000000  0.000030\n",
      "17   0.955014  0.044814  0.0  0.000000  0.000173  0.000000\n",
      "18   0.565425  0.434565  0.0  0.000000  0.000007  0.000003\n",
      "19   0.000000  0.000000  0.0  0.000000  1.000000  0.000000\n",
      "20   0.000000  0.000000  0.0  0.000000  0.000000  0.000000\n",
      "21   0.600163  0.399837  0.0  0.000000  0.000000  0.000000\n",
      "22   0.000000  0.000000  0.0  0.000000  0.000000  0.000000\n",
      "23   0.654176  0.345824  0.0  0.000000  0.000000  0.000000\n",
      "24   0.375204  0.624796  0.0  0.000000  0.000000  0.000000\n",
      "25   0.511094  0.488906  0.0  0.000000  0.000000  0.000000\n",
      "26   0.280751  0.719249  0.0  0.000000  0.000000  0.000000\n",
      "27   0.293605  0.706395  0.0  0.000000  0.000000  0.000000\n",
      "28   0.418277  0.581711  0.0  0.000000  0.000009  0.000003\n",
      "29   0.310530  0.689466  0.0  0.000000  0.000000  0.000004\n",
      "..        ...       ...  ...       ...       ...       ...\n",
      "77   0.040033  0.959967  0.0  0.000000  0.000000  0.000000\n",
      "78   0.000000  0.000000  0.0  0.000000  0.000000  0.000000\n",
      "79   1.000000  0.000000  0.0  0.000000  0.000000  0.000000\n",
      "80   0.999009  0.000000  0.0  0.000991  0.000000  0.000000\n",
      "81   0.315197  0.684616  0.0  0.000000  0.000179  0.000007\n",
      "82   0.014386  0.985598  0.0  0.000000  0.000000  0.000016\n",
      "83   0.473899  0.526098  0.0  0.000000  0.000000  0.000003\n",
      "84   0.297766  0.702192  0.0  0.000000  0.000041  0.000000\n",
      "85   0.332354  0.667644  0.0  0.000000  0.000000  0.000002\n",
      "86   0.921471  0.078243  0.0  0.000000  0.000286  0.000000\n",
      "87   0.000000  0.000000  0.0  0.000000  0.000000  0.000000\n",
      "88   0.904837  0.095152  0.0  0.000000  0.000009  0.000001\n",
      "89   0.000000  0.000000  0.0  0.000000  0.000000  0.000000\n",
      "90   0.332354  0.667644  0.0  0.000000  0.000000  0.000002\n",
      "91   0.332354  0.667644  0.0  0.000000  0.000000  0.000002\n",
      "92   1.000000  0.000000  0.0  0.000000  0.000000  0.000000\n",
      "93   0.149764  0.850231  0.0  0.000000  0.000000  0.000005\n",
      "94   0.202785  0.797012  0.0  0.000000  0.000198  0.000005\n",
      "95   0.297766  0.702192  0.0  0.000000  0.000041  0.000000\n",
      "96   0.418277  0.581711  0.0  0.000000  0.000009  0.000003\n",
      "97   0.014836  0.985164  0.0  0.000000  0.000000  0.000000\n",
      "98   0.332354  0.667644  0.0  0.000000  0.000000  0.000002\n",
      "99   0.000000  0.000000  0.0  0.000000  1.000000  0.000000\n",
      "100  0.026992  0.972998  0.0  0.000000  0.000006  0.000005\n",
      "101  0.418277  0.581711  0.0  0.000000  0.000009  0.000003\n",
      "102  0.904837  0.095152  0.0  0.000000  0.000009  0.000001\n",
      "103  0.904837  0.095152  0.0  0.000000  0.000009  0.000001\n",
      "104  0.450463  0.549537  0.0  0.000000  0.000000  0.000000\n",
      "105  1.000000  0.000000  0.0  0.000000  0.000000  0.000000\n",
      "106  0.463835  0.535949  0.0  0.000000  0.000216  0.000000\n",
      "\n",
      "[107 rows x 6 columns]\n",
      "Testing time (5, 'equal-width'): 0.41 s.\n",
      "Training time (5, 'equal-size'): 0.15 s.\n",
      "            1         2    3         5         6             7\n",
      "0    1.000000  0.000000  0.0  0.000000  0.000000  0.000000e+00\n",
      "1    0.326259  0.673741  0.0  0.000000  0.000000  0.000000e+00\n",
      "2    0.064091  0.935909  0.0  0.000000  0.000000  0.000000e+00\n",
      "3    0.060075  0.939925  0.0  0.000000  0.000000  0.000000e+00\n",
      "4    1.000000  0.000000  0.0  0.000000  0.000000  0.000000e+00\n",
      "5    0.154415  0.845585  0.0  0.000000  0.000000  5.190414e-07\n",
      "6    1.000000  0.000000  0.0  0.000000  0.000000  0.000000e+00\n",
      "7    0.763433  0.236567  0.0  0.000000  0.000000  0.000000e+00\n",
      "8    0.732248  0.267751  0.0  0.000000  0.000000  7.553563e-07\n",
      "9    0.932018  0.067982  0.0  0.000000  0.000000  0.000000e+00\n",
      "10   0.005724  0.000000  0.0  0.000000  0.994276  0.000000e+00\n",
      "11   0.011206  0.000000  0.0  0.000000  0.988794  0.000000e+00\n",
      "12   1.000000  0.000000  0.0  0.000000  0.000000  0.000000e+00\n",
      "13   0.011056  0.988892  0.0  0.000000  0.000000  5.202917e-05\n",
      "14   0.059534  0.940416  0.0  0.000000  0.000051  0.000000e+00\n",
      "15   0.007551  0.992443  0.0  0.000000  0.000000  6.091865e-06\n",
      "16   0.025423  0.974532  0.0  0.000000  0.000000  4.486444e-05\n",
      "17   1.000000  0.000000  0.0  0.000000  0.000000  0.000000e+00\n",
      "18   1.000000  0.000000  0.0  0.000000  0.000000  1.031557e-07\n",
      "19   0.017810  0.000000  0.0  0.000000  0.982190  0.000000e+00\n",
      "20   0.929874  0.000000  0.0  0.000000  0.070126  0.000000e+00\n",
      "21   0.285111  0.714874  0.0  0.000000  0.000000  1.521200e-05\n",
      "22   0.787307  0.212693  0.0  0.000000  0.000000  0.000000e+00\n",
      "23   0.203605  0.796395  0.0  0.000000  0.000000  4.888473e-07\n",
      "24   0.315259  0.684741  0.0  0.000000  0.000000  4.415391e-07\n",
      "25   0.267076  0.732924  0.0  0.000000  0.000000  0.000000e+00\n",
      "26   0.099736  0.900264  0.0  0.000000  0.000000  0.000000e+00\n",
      "27   0.132732  0.867264  0.0  0.000000  0.000000  3.470126e-06\n",
      "28   0.999977  0.000000  0.0  0.000000  0.000000  2.310635e-05\n",
      "29   0.049245  0.950746  0.0  0.000000  0.000000  9.196000e-06\n",
      "..        ...       ...  ...       ...       ...           ...\n",
      "77   0.568000  0.432000  0.0  0.000000  0.000000  0.000000e+00\n",
      "78   0.492712  0.507288  0.0  0.000000  0.000000  0.000000e+00\n",
      "79   0.467753  0.532247  0.0  0.000000  0.000000  0.000000e+00\n",
      "80   0.964596  0.035179  0.0  0.000224  0.000000  0.000000e+00\n",
      "81   0.166687  0.833313  0.0  0.000000  0.000000  0.000000e+00\n",
      "82   0.011056  0.988892  0.0  0.000000  0.000000  5.202917e-05\n",
      "83   0.649979  0.350021  0.0  0.000000  0.000000  2.528707e-07\n",
      "84   0.050866  0.949134  0.0  0.000000  0.000000  0.000000e+00\n",
      "85   0.143068  0.856895  0.0  0.000000  0.000000  3.740330e-05\n",
      "86   0.536060  0.463940  0.0  0.000000  0.000000  0.000000e+00\n",
      "87   1.000000  0.000000  0.0  0.000000  0.000000  0.000000e+00\n",
      "88   0.900648  0.099352  0.0  0.000000  0.000000  0.000000e+00\n",
      "89   1.000000  0.000000  0.0  0.000000  0.000000  0.000000e+00\n",
      "90   0.202057  0.797940  0.0  0.000000  0.000000  3.001443e-06\n",
      "91   0.680615  0.319384  0.0  0.000000  0.000000  3.861516e-07\n",
      "92   0.993362  0.006404  0.0  0.000047  0.000187  0.000000e+00\n",
      "93   0.012181  0.987771  0.0  0.000000  0.000000  4.777058e-05\n",
      "94   0.488488  0.511498  0.0  0.000000  0.000000  1.478115e-05\n",
      "95   0.816740  0.183260  0.0  0.000000  0.000000  0.000000e+00\n",
      "96   0.769644  0.230256  0.0  0.000000  0.000099  5.292878e-07\n",
      "97   0.728137  0.271863  0.0  0.000000  0.000000  0.000000e+00\n",
      "98   0.041377  0.958623  0.0  0.000000  0.000000  0.000000e+00\n",
      "99   0.023066  0.000000  0.0  0.000000  0.976934  0.000000e+00\n",
      "100  0.022889  0.977090  0.0  0.000000  0.000000  2.094410e-05\n",
      "101  0.520440  0.479560  0.0  0.000000  0.000000  0.000000e+00\n",
      "102  1.000000  0.000000  0.0  0.000000  0.000000  0.000000e+00\n",
      "103  0.993350  0.006650  0.0  0.000000  0.000000  0.000000e+00\n",
      "104  0.157946  0.842054  0.0  0.000000  0.000000  0.000000e+00\n",
      "105  0.779022  0.220978  0.0  0.000000  0.000000  0.000000e+00\n",
      "106  0.832064  0.165216  0.0  0.000000  0.002719  0.000000e+00\n",
      "\n",
      "[107 rows x 6 columns]\n",
      "Testing time (5, 'equal-size'): 0.66 s.\n",
      "Training time (10, 'equal-width'): 0.15 s.\n",
      "            1         2    3    5         6             7\n",
      "0    0.826241  0.173759  0.0  0.0  0.000000  0.000000e+00\n",
      "1    0.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "2    0.000000  0.999845  0.0  0.0  0.000000  1.549022e-04\n",
      "3    0.051583  0.948417  0.0  0.0  0.000000  0.000000e+00\n",
      "4    1.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "5    0.019428  0.980572  0.0  0.0  0.000000  0.000000e+00\n",
      "6    0.994792  0.005208  0.0  0.0  0.000000  0.000000e+00\n",
      "7    0.973723  0.026277  0.0  0.0  0.000000  0.000000e+00\n",
      "8    0.311343  0.688657  0.0  0.0  0.000000  0.000000e+00\n",
      "9    0.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "10   0.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "11   0.000000  0.000000  0.0  0.0  1.000000  0.000000e+00\n",
      "12   0.911223  0.088777  0.0  0.0  0.000000  0.000000e+00\n",
      "13   0.000000  0.999793  0.0  0.0  0.000000  2.065256e-04\n",
      "14   0.071538  0.928462  0.0  0.0  0.000000  0.000000e+00\n",
      "15   0.165865  0.834089  0.0  0.0  0.000000  4.590792e-05\n",
      "16   0.000000  1.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "17   1.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "18   0.512803  0.487196  0.0  0.0  0.000000  1.014637e-06\n",
      "19   0.000000  0.000000  0.0  0.0  1.000000  0.000000e+00\n",
      "20   0.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "21   0.106247  0.893753  0.0  0.0  0.000000  0.000000e+00\n",
      "22   0.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "23   1.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "24   0.185398  0.814602  0.0  0.0  0.000000  0.000000e+00\n",
      "25   0.194186  0.805814  0.0  0.0  0.000000  0.000000e+00\n",
      "26   0.157626  0.842374  0.0  0.0  0.000000  0.000000e+00\n",
      "27   0.140078  0.859922  0.0  0.0  0.000000  0.000000e+00\n",
      "28   0.413214  0.586714  0.0  0.0  0.000066  5.606336e-06\n",
      "29   0.000000  1.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "..        ...       ...  ...  ...       ...           ...\n",
      "77   0.000000  1.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "78   0.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "79   1.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "80   0.000000  0.000000  0.0  1.0  0.000000  0.000000e+00\n",
      "81   1.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "82   0.000000  0.999912  0.0  0.0  0.000000  8.778379e-05\n",
      "83   0.090135  0.909864  0.0  0.0  0.000000  1.212727e-06\n",
      "84   0.030352  0.969648  0.0  0.0  0.000000  0.000000e+00\n",
      "85   0.073680  0.926299  0.0  0.0  0.000000  2.039324e-05\n",
      "86   0.454190  0.545810  0.0  0.0  0.000000  0.000000e+00\n",
      "87   0.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "88   0.370036  0.629964  0.0  0.0  0.000000  0.000000e+00\n",
      "89   0.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "90   0.207846  0.792154  0.0  0.0  0.000000  0.000000e+00\n",
      "91   0.123247  0.876751  0.0  0.0  0.000000  1.759762e-06\n",
      "92   0.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "93   0.036675  0.963316  0.0  0.0  0.000000  8.378550e-06\n",
      "94   0.051173  0.948795  0.0  0.0  0.000022  1.031531e-05\n",
      "95   0.103337  0.896663  0.0  0.0  0.000000  0.000000e+00\n",
      "96   0.413214  0.586714  0.0  0.0  0.000066  5.606336e-06\n",
      "97   0.000000  1.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "98   0.075429  0.924570  0.0  0.0  0.000000  8.698795e-07\n",
      "99   0.000000  0.000000  0.0  0.0  1.000000  0.000000e+00\n",
      "100  0.000000  0.999974  0.0  0.0  0.000000  2.582036e-05\n",
      "101  0.298505  0.701495  0.0  0.0  0.000000  0.000000e+00\n",
      "102  1.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "103  0.991060  0.008940  0.0  0.0  0.000000  0.000000e+00\n",
      "104  0.054441  0.945559  0.0  0.0  0.000000  0.000000e+00\n",
      "105  0.000000  0.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "106  0.000000  1.000000  0.0  0.0  0.000000  0.000000e+00\n",
      "\n",
      "[107 rows x 6 columns]\n",
      "Testing time (10, 'equal-width'): 0.23 s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (10, 'equal-size'): 0.23 s.\n",
      "            1         2    3    5         6         7\n",
      "0    1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "1    1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "2    0.023109  0.976891  0.0  0.0  0.000000  0.000000\n",
      "3    0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "4    1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "5    0.055174  0.944826  0.0  0.0  0.000000  0.000000\n",
      "6    1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "7    0.907925  0.092075  0.0  0.0  0.000000  0.000000\n",
      "8    0.881679  0.118321  0.0  0.0  0.000000  0.000000\n",
      "9    1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "10   1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "11   0.000000  0.000000  0.0  0.0  1.000000  0.000000\n",
      "12   1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "13   0.000000  0.999936  0.0  0.0  0.000000  0.000064\n",
      "14   0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "15   0.000000  0.999997  0.0  0.0  0.000000  0.000003\n",
      "16   0.000000  0.999872  0.0  0.0  0.000000  0.000128\n",
      "17   1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "18   1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "19   0.000130  0.000000  0.0  0.0  0.999870  0.000000\n",
      "20   0.003845  0.000000  0.0  0.0  0.996155  0.000000\n",
      "21   0.943713  0.056287  0.0  0.0  0.000000  0.000000\n",
      "22   0.117438  0.882562  0.0  0.0  0.000000  0.000000\n",
      "23   0.443946  0.556054  0.0  0.0  0.000000  0.000000\n",
      "24   0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "25   0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "26   0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "27   0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "28   1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "29   0.847966  0.151728  0.0  0.0  0.000000  0.000306\n",
      "..        ...       ...  ...  ...       ...       ...\n",
      "77   0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "78   0.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "79   1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "80   0.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "81   0.646003  0.353997  0.0  0.0  0.000000  0.000000\n",
      "82   0.000000  0.999885  0.0  0.0  0.000000  0.000115\n",
      "83   0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "84   0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "85   0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "86   0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "87   1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "88   0.931145  0.068855  0.0  0.0  0.000000  0.000000\n",
      "89   1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "90   0.872470  0.127439  0.0  0.0  0.000000  0.000091\n",
      "91   0.096210  0.903790  0.0  0.0  0.000000  0.000000\n",
      "92   1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "93   0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "94   0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "95   0.906187  0.093813  0.0  0.0  0.000000  0.000000\n",
      "96   0.908253  0.091415  0.0  0.0  0.000332  0.000000\n",
      "97   0.068030  0.931970  0.0  0.0  0.000000  0.000000\n",
      "98   0.030113  0.969887  0.0  0.0  0.000000  0.000000\n",
      "99   0.000000  0.000000  0.0  0.0  1.000000  0.000000\n",
      "100  0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "101  0.000000  1.000000  0.0  0.0  0.000000  0.000000\n",
      "102  1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "103  0.996507  0.003493  0.0  0.0  0.000000  0.000000\n",
      "104  0.142610  0.857390  0.0  0.0  0.000000  0.000000\n",
      "105  1.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "106  0.000000  0.000000  0.0  0.0  1.000000  0.000000\n",
      "\n",
      "[107 rows x 6 columns]\n",
      "Testing time (10, 'equal-size'): 0.25 s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Brier score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>equal-width</th>\n",
       "      <td>0.336449</td>\n",
       "      <td>1.100715</td>\n",
       "      <td>0.462449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal-size</th>\n",
       "      <td>0.271028</td>\n",
       "      <td>1.158733</td>\n",
       "      <td>0.427947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>equal-width</th>\n",
       "      <td>0.242991</td>\n",
       "      <td>1.184068</td>\n",
       "      <td>0.427942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal-size</th>\n",
       "      <td>0.196262</td>\n",
       "      <td>1.340014</td>\n",
       "      <td>0.355557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10</th>\n",
       "      <th>equal-width</th>\n",
       "      <td>0.177570</td>\n",
       "      <td>1.373174</td>\n",
       "      <td>0.419812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal-size</th>\n",
       "      <td>0.168224</td>\n",
       "      <td>1.564614</td>\n",
       "      <td>0.335947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Accuracy  Brier score       AUC\n",
       "3  equal-width  0.336449     1.100715  0.462449\n",
       "   equal-size   0.271028     1.158733  0.427947\n",
       "5  equal-width  0.242991     1.184068  0.427942\n",
       "   equal-size   0.196262     1.340014  0.355557\n",
       "10 equal-width  0.177570     1.373174  0.419812\n",
       "   equal-size   0.168224     1.564614  0.335947"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your code (leave this part unchanged, except for if auc is undefined)\n",
    "\n",
    "glass_train_df = pd.read_csv(\"glass_train.txt\")\n",
    "\n",
    "glass_test_df = pd.read_csv(\"glass_test.txt\")\n",
    "\n",
    "nb_model = NaiveBayes()\n",
    "\n",
    "test_labels = glass_test_df[\"CLASS\"]\n",
    "\n",
    "nobins_values = [3,5,10]\n",
    "bintype_values = [\"equal-width\",\"equal-size\"]\n",
    "parameters = [(nobins,bintype) for nobins in nobins_values for bintype in bintype_values]\n",
    "\n",
    "results = np.empty((len(parameters),3))\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    t0 = time.perf_counter()\n",
    "    nb_model.fit(glass_train_df,nobins=parameters[i][0],bintype=parameters[i][1])\n",
    "    print(\"Training time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = nb_model.predict(glass_test_df)\n",
    "    print(\"Testing time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n",
    "                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n",
    "\n",
    "results = pd.DataFrame(results,index=pd.MultiIndex.from_product([nobins_values,bintype_values]),\n",
    "                       columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.85\n",
      "AUC on training set: 0.97\n",
      "Brier score on training set: 0.23\n"
     ]
    }
   ],
   "source": [
    "train_labels = glass_train_df[\"CLASS\"]\n",
    "nb_model.fit(glass_train_df)\n",
    "predictions = nb_model.predict(glass_train_df)\n",
    "print(\"Accuracy on training set: {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
    "print(\"AUC on training set: {0:.2f}\".format(auc(predictions,train_labels)))\n",
    "print(\"Brier score on training set: {0:.2f}\".format(brier_score(predictions,train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on assumptions, things that do not work properly, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
